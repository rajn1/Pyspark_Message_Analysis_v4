{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pyspark\n",
    "import findspark\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "\n",
    "sc = SparkSession \\\n",
    "        .builder \\\n",
    "        .master('spark://10.0.0.118:7077') \\\n",
    "        .appName(\"sparkFromJupyter\") \\\n",
    "        .config(\"spark.rpc.message.maxSize\", 2040) \\\n",
    "        .getOrCreate()\n",
    "sqlContext = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)\n",
    "print(\"Spark Version: \" + sc.version)\n",
    "print(\"PySpark Version: \" + pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.hackdeploy.com/how-to-run-pyspark-in-a-jupyter-notebook/\n",
    "# ^ Set-up details for Pyspark + Jupyter\n",
    "\n",
    "# https://phoenixnap.com/kb/install-spark-on-windows-10\n",
    "# ^ Details on installing Pyspark\n",
    "\n",
    "# https://stackoverflow.com/questions/69669524/spark-illegal-character-in-path\n",
    "# ^ Details on fixing acccess to Spark Cluster in cmd\n",
    "\n",
    "# spark-shell --master spark://10.0.0.118:7077\n",
    "# ^ How to kick off spark-shell in cmd\n",
    "\n",
    "# https://www.sicara.ai/blog/2017-05-02-get-started-pyspark-jupyter-notebook-3-minutes\n",
    "# ^ Details on Findspark\n",
    "\n",
    "# https://www.geeksforgeeks.org/python-xml-to-json/\n",
    "# ^ XML to JSON\n",
    "\n",
    "# https://stackoverflow.com/questions/49675860/pyspark-converting-json-string-to-dataframe\n",
    "# ^ Resource for converting JSON into dataframe using parallelize\n",
    "\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-window-functions/\n",
    "# ^ Pyspark Window function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row, Window\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import json, xmltodict\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Sparkcontext instance for analysis\n",
    "findspark.init()\n",
    "sc = pyspark.SparkContext(appName=\"SMS\")\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import in the SMS XML file provided y the SMS Backup and Restore app\n",
    "# Parse XML file into Dictionary\n",
    "\n",
    "# Cols in Data: Protocol, Address (sender), date, type, subject, body, toa, service_center, read, \n",
    "# status, locked_ date_sent, sub_id, readable_date, contact_name\n",
    "\n",
    "# All column names are prepended with an '@'\n",
    "\n",
    "# Time to Run: 2 Minutes\n",
    "\n",
    "with open(r\"/Users/rajn/Library/CloudStorage/GoogleDrive-kinqraj@gmail.com/My Drive/Backup/SMS-12_24_22-NoMedia.xml\",encoding=\"utf8\") as xml_file:\n",
    "    data_dict = xmltodict.parse(xml_file.read())\n",
    "    \n",
    "xml_file.close()\n",
    "\n",
    "# Convert Dictionary into parsable JSON file\n",
    "\n",
    "json_data = json.dumps(data_dict['smses']['sms'])\n",
    "\n",
    "with open(\"sms_data.json\", \"w\") as json_file:\n",
    "        json_file.write(json_data)\n",
    "        \n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Originally I used a for loop to convert the dict I had to a df, but it was extraordinarily slow (gave up after 1 hour of run)\n",
    "\n",
    "#for key in data_dict['smses']['sms']:\n",
    "#         newRow = spark.createDataFrame([(key['@contact_name'], key['@readable_date'], key['@body'])], columns)\n",
    "#         sms_df.union(newRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert JSON data into dataframe\n",
    "# 44 Seconds\n",
    "\n",
    "sms_df = spark.read.json(\"sms_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save space and remove uneeded / unknown cols\n",
    "sms_df = sms_df.drop(\"@service_center\",\"@status\",\"@sub_id\",\"@subject\",\"@toa\",\"@type\", \"@locked\",\"@protocol\", \"@read\", \"@sc_toa\")\n",
    "\n",
    "# Rule defining sent_by_me column \n",
    "sent_by_me_col = F.when(F.col('@date_sent') == 0, 'Sent By Raj').otherwise('Sent By Contact')\n",
    "\n",
    "# Add timestamp field to better group with\n",
    "sms_df = sms_df.withColumn(\"@timestamp_sent\", F.to_timestamp(\"@readable_date\", 'MMM d, yyyy HH:mm:ss'))\n",
    "sms_df = sms_df.withColumn(\"@timestamp_year\", F.year(\"@timestamp_sent\"))\n",
    "sms_df = sms_df.withColumn(\"@timestamp_weekday\", F.date_format(F.col(\"@timestamp_sent\"), 'E'))\n",
    "sms_df = sms_df.withColumn(\"@sent_by_me\", sent_by_me_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_count_pivot_df = sms_df.groupBy('@contact_name').pivot('@timestamp_year').count()\n",
    "sms_count_df = sms_df.groupBy('@contact_name', '@timestamp_year').count()\n",
    "# sms_count_pivot_df.show(1000)\n",
    "sms_count_df.show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a window to determine, for each contact, which year I messaged them most\n",
    "contactCountWindowSpec = Window.partitionBy(\"@contact_name\").orderBy(\"count\")\n",
    "\n",
    "sms_window_contact_year_count_df = sms_count_df.withColumn(\"rank_year\", F.row_number().over(contactCountWindowSpec)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a window to determine which contact + year combo I messaged most\n",
    "countWindowSpec = Window.partitionBy().orderBy(F.col(\"count\").desc())\n",
    "\n",
    "sms_window_contact_year_count_df = sms_count_df.withColumn(\"rank_year\", F.row_number().over(countWindowSpec)).show(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a window to determine, for each year, who I messaged most\n",
    "countWindowSpec = Window.partitionBy('@timestamp_year').orderBy(F.col(\"count\").desc())\n",
    "\n",
    "sms_window_contact_year_count_df = sms_count_df.withColumn(\"rank_year\", F.row_number().over(countWindowSpec)).filter(F.col(\"rank_year\") <= 10).show(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEAS\n",
    "# Percent of total messages for a given year\n",
    "# Merge in FB texts by name match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to an @date_sent of 0, which represents messages from me in this dataset, to find which year I sent the most messages\n",
    "\n",
    "countWindowSpec = Window.partitionBy().orderBy(F.col('count').desc())\n",
    "\n",
    "sms_count_pivot_df = sms_df.filter(sms_df['@date_sent'] == 0).groupBy('@timestamp_year').count()\n",
    "\n",
    "sms_count_pivot_df.withColumn(\"rank_sends_year\", F.row_number().over(countWindowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By person, did I send more messages or do they?\n",
    "\n",
    "countWindowSpec = Window.partitionBy().orderBy(F.col('Sent By Raj').desc())\n",
    "\n",
    "sms_count_df = sms_df.groupBy('@contact_name').pivot('@sent_by_me').count()\n",
    "\n",
    "sms_count_df.withColumn(\"rank_sends_year\", F.row_number().over(countWindowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By year, did I send more messages or do they?\n",
    "\n",
    "countWindowSpec = Window.partitionBy().orderBy(F.col('Sent By Raj').desc())\n",
    "\n",
    "sms_count_df = sms_df.filter(sms_df['@contact_name'] != '(unknown)').groupBy('@timestamp_year').pivot('@sent_by_me').count()\n",
    "\n",
    "sms_count_df.withColumn(\"rank_sends_year\", F.row_number().over(countWindowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By year AND contact, did I send more messages or do they?\n",
    "\n",
    "countWindowSpec = Window.partitionBy().orderBy(F.col('Sent By Raj').desc())\n",
    "\n",
    "sms_count_df = sms_df.filter(sms_df['@contact_name'] != '(unknown)').groupBy('@timestamp_year', '@contact_name').pivot('@sent_by_me').count()\n",
    "\n",
    "sms_count_df.withColumn(\"rank_sends_year\", F.row_number().over(countWindowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who have I messaged the most overall, by year ranked by 2021\n",
    "\n",
    "\n",
    "countWindowYearSpec2022 = Window.partitionBy().orderBy(F.col('2022').desc())\n",
    "countWindowYearSpec2021 = Window.partitionBy().orderBy(F.col('2021').desc())\n",
    "countWindowYearSpec2020 = Window.partitionBy().orderBy(F.col('2020').desc())\n",
    "countWindowYearSpec2019 = Window.partitionBy().orderBy(F.col('2019').desc())\n",
    "countWindowYearSpec2018 = Window.partitionBy().orderBy(F.col('2018').desc())\n",
    "\n",
    "sms_df.groupBy('@contact_name').pivot('@timestamp_year').count()\\\n",
    ".withColumn('rank_2018', F.row_number().over(countWindowYearSpec2018))\\\n",
    ".withColumn('rank_2019', F.row_number().over(countWindowYearSpec2019))\\\n",
    ".withColumn('rank_2020', F.row_number().over(countWindowYearSpec2020))\\\n",
    ".withColumn('rank_2021', F.row_number().over(countWindowYearSpec2021))\\\n",
    ".withColumn('rank_2022', F.row_number().over(countWindowYearSpec2022))\\\n",
    ".show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who have I messaged the most overall, by weekday ranked by Fri, in 2021\n",
    "\n",
    "Year = 2022\n",
    "# Weekdays: Mon, Tue, Wed, Thu, Fri, Sat, Sun\n",
    "Weekday = 'Fri'\n",
    "\n",
    "countWindowSpecDay = Window.partitionBy().orderBy(F.col(Weekday).desc())\n",
    "\n",
    "sms_df.filter(F.col('@timestamp_year') == Year)\\\n",
    ".groupBy('@contact_name').pivot('@timestamp_weekday').count()\\\n",
    ".withColumn('rank_Day', F.row_number().over(countWindowSpecDay))\\\n",
    ".show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any trend in what weekday I message most on generally? By percent of that year's messages\n",
    "\n",
    "countWindowSpecDay = Window.partitionBy().orderBy(F.col(Weekday).desc())\n",
    "\n",
    "sms_df.groupBy('@timestamp_weekday', '@timestamp_year')\\\n",
    ".count()\\\n",
    ".withColumn('percent_of_year', F.col('count')/F.sum('count').over(Window.partitionBy('@timestamp_year')) * 100)\\\n",
    ".orderBy([F.col('@timestamp_year'), F.col('percent_of_year').desc()])\\\n",
    ".show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
